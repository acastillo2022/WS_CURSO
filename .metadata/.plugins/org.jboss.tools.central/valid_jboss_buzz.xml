<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Open source edge detection with OpenCV and Pachyderm</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/06/01/open-source-edge-detection-opencv-and-pachyderm" /><author><name>JooHo Lee</name></author><id>23f2b120-e095-4b1a-9819-d775cccd86b6</id><updated>2022-06-01T07:00:00Z</updated><published>2022-06-01T07:00:00Z</published><summary type="html">&lt;p&gt;Edge detection is central to image recognition, which is one of the most common applications of &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt;. This article introduces a &lt;a href="https://jupyter.org"&gt;Jupyter&lt;/a&gt; notebook for creating a &lt;a href="https://www.pachyderm.com/"&gt;Pachyderm&lt;/a&gt; pipeline that performs edge detection. For convenience, the article uses a &lt;a href="openshift"&gt;Red Hat OpenShift&lt;/a&gt; cluster configuration described in an earlier Red Hat Developer article, &lt;a href="https://developers.redhat.com/articles/2022/05/05/how-install-open-source-tool-creating-machine-learning-pipelines"&gt;How to install an open source tool for creating machine learning pipelines&lt;/a&gt;, but you can use the notebook on any &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster.&lt;/p&gt; &lt;h2&gt;Edge detection with OpenCV&lt;/h2&gt; &lt;p&gt;A good way to understand edge detection is to look at Figure 1. Compare the picture my son drew of the cartoon character Sherek, on the left, with the image produced by an edge detection algorithm on the right. Edge detection is one of the first steps in many machine learning processes that alter images or identify their content.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp1_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp1_0.png?itok=67G3aI9c" width="827" height="366" alt="Edge detection, performed here on a child's picture, is the first step in identifying the elements an image." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Edge detection, performed here on a child's picture, is the first step in identifying the elements an image. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The most popular open source tool for image and video manipulation is &lt;a href="https://opencv.org/releases/"&gt;OpenCV&lt;/a&gt;. I use its edge detection library along with Pachyderm to create the machine learning pipeline in the Jupyter notebook.&lt;/p&gt; &lt;p&gt;There are several advantages to using Pachyderm for this task. Like Git, Pachyderm's data versioning allows you to manage your data and iterate over it using repositories and commits. Pachyderm is not limited to text files and structured data, but can version any data (image, audio, video, text). Pachyderm's version control system is optimized to scale to large datasets of all types, providing consistent reproducibility.&lt;/p&gt; &lt;p&gt;Pachyderm's pipelines allow you to connect your code to data repositories. It can be used to automate many components of the machine learning lifecycle, such as data preparation, testing, model training, and more, by rerunning the pipeline when new data is committed. Pachyderm's pipelines and version control capabilities work together to visualize the end-to-end flow of your machine learning workflow.&lt;/p&gt; &lt;p&gt;The notebook in this article creates two repositories. The first, named &lt;code&gt;images&lt;/code&gt;, receives the input images. The second, named &lt;code&gt;edges&lt;/code&gt;, stores the results of the &lt;a href="https://github.com/pachyderm/pachyderm/blob/master/examples/opencv/edges.json"&gt;Pachyderm pipeline&lt;/a&gt; (Figure 2). Once the flow is configured, execution of the pipeline is triggered by committing an image to the &lt;code&gt;images&lt;/code&gt; source repository. The Python source code I use for edge detection is in a &lt;a href="https://github.com/pachyderm/pachyderm/blob/master/examples/opencv/edges.py"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp11.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp11.png?itok=aUzlZam5" width="1191" height="635" alt="Images in an input Pachyderm repository pass through the edges.py pipeline to generate edges in an output Pachyderm repository." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Images in an input Pachyderm repository pass through the edges.py pipeline to generate edges in an output Pachyderm repository. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Pachyderm's pipeline monitors the &lt;code&gt;images&lt;/code&gt; source repository and detects when a new image is pushed to it. Once the pipeline pod is running, you can reuse it by pushing other images to the source repository. The notebook includes detailed explanations for each of its cells.&lt;/p&gt; &lt;h2&gt;Obtaining and running the Jupyter notebook&lt;/h2&gt; &lt;p&gt;The environment in &lt;a href="https://developers.redhat.com/articles/2022/01/04/easiest-way-install-pachyderm-opendatahub"&gt;my previous article&lt;/a&gt; went to &lt;a href="https://opendatahub.io"&gt;Open Data Hub to &lt;/a&gt;download Pachyderm and JupyterHub on an OpenShift instance. The steps in this article start with that environment. Using Open Data Hub, you can also deploy Pachyderm and JupyterHub on any Kubernetes cluster you have.&lt;/p&gt; &lt;p&gt;First, visit your installed instance of JupyterHub. If you installed JupyterHub through Open Data Hub, you can find a &lt;code&gt;jupyterhub&lt;/code&gt; route in the Open Data Hub project (Figure 3).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp2.png?itok=uWOjyqaX" width="863" height="617" alt="Choose Routes in the left-hand menu and then click the jupyterhub route." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Choose Routes in the left-hand menu and then click the jupyterhub route. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;OpenShift's OAuth proxy is integrated with JupyterHub, so you can get into JupyterHub after you enter your username and password in OpenShift (Figure 4). The web page prompts you to give JupyterHub access to your account (Figure 5).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp3.png?itok=5LXzUhGS" width="540" height="609" alt="Log in to OpenShift Container Platform to get access to JupyterHub." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Log in to OpenShift Container Platform to get access to JupyterHub. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp4.png?itok=MOsUtG-V" width="858" height="361" alt="Press "Allow selected permissions" to giv e JupyterHub access to your information." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Press "Allow selected permissions" to giv e JupyterHub access to your information. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Click "Allow selected permissions" to give JupyterHub access to your information.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;We turn now to the JupyterHub web interface, Swan. On the &lt;strong&gt;Start a notebook server&lt;/strong&gt; page, choose the &lt;strong&gt;Standard Data Science&lt;/strong&gt; notebook (Figure 6).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp5.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp5.png?itok=aFuxfseU" width="625" height="529" alt="Start the server for the Standard Data Science notebook." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Start the server for the Standard Data Science notebook. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The image takes a few minutes to load (Figure 7).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp6.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp6.png?itok=odiUSn4B" width="625" height="529" alt="A pop-up shows the progress while the image is being loaded." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: A pop-up shows the progress while the image is being loaded. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Using the menu at the top of the interface (Figure 8), clone the GitHub repository, which is named &lt;a href="https://github.com/Jooho/pachyderm-operator-manifests.git"&gt;https://github.com/Jooho/pachyderm-operator-manifests.git&lt;/a&gt; (Figure 9).&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp7.png?itok=SPvllC67" width="866" height="743" alt="The icon to clone the GitHub repository is the rightmost icon on the top menu on the left of the screen." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: The icon to clone the GitHub repository is the rightmost icon on the top menu on the left of the screen. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp8.png?itok=_xCU50JJ" width="929" height="802" alt="A dialog allows you to paste in the URL of the Git repository." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: A dialog allows you to paste in the URL of the Git repository. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Run the notebook by opening the file on your system (Figure 10). The notebook is at &lt;code&gt;/pachyderm-operator-manifests/notebooks/pachyderm-opencv.ipynb&lt;/code&gt; in the repository you downloaded from GitHub (Figure 11). You can now interact with the cells in the &lt;a href="https://github.com/Jooho/pachyderm-operator-manifests/blob/master/notebooks/pachyderm-opencv.ipynb"&gt;OpenCV Edge Detection Jupyter notebook&lt;/a&gt;.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp9.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp9.png?itok=LGl9xRsA" width="388" height="133" alt="From the File menu, choose "Open from Path."" loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: From the File menu, choose "Open from Path." &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jp10.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/jp10.png?itok=U3t6V_38" width="863" height="464" alt="In the dialog box, paste in the path to the local notebook." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 11: In the dialog box, paste in the path to the local notebook. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Congratulations: You're now ready to start experimenting with image recognition, which has a number of use cases in machine learning. The following two-minute video outlines the steps in this article so you can see how it works in action!&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/06/01/open-source-edge-detection-opencv-and-pachyderm" title="Open source edge detection with OpenCV and Pachyderm"&gt;Open source edge detection with OpenCV and Pachyderm&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>JooHo Lee</dc:creator><dc:date>2022-06-01T07:00:00Z</dc:date></entry><entry><title type="html">Kafka Optimization Theorem</title><link rel="alternate" href="http://www.ofbizian.com/2022/06/kafka-optimization-theorem.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/06/kafka-optimization-theorem.html</id><updated>2022-06-01T04:51:00Z</updated><content type="html">One of the foundational theorems in distributed systems is the . It simply explains why a distributed data store can provide only two (CP or AP, since P is a given) of the consistency (C), availability(A), and partition tolerance(P) guarantees. While this dilemma explains the main tradeoffs of distributed systems in the case of network partitions, it is not useful in the day-to-day optimizing of systems for specific performance goals in the happy path scenarios. In addition to consistency and availability, developers have to optimize applications for latency and throughput too. In this post, we will uncover how the main distributed systems forces relate to each other and which are the main primitives influencing these forces in the context of Apache Kafka. We will visualize and formulate these findings in the form of a theorem that can serve as a guide while optimizing Apache Kafka for specific goals. KAFKA PRIMITIVES One of the less-known distributed systems theorems is which extends on CAP by explaining what else (E) happens when a distributed system is running normally in the absence of partitioning. In that case, one can optimize for latency (L) or consistency (C) (hence the full acronym PAC-E-LC). PACELC explains how some systems such as Amazon’s Dynamo favor availability and lower latency over consistency, and how ACID-compliant systems favor consistency over availability or lower latency.   PACELC theorem for distributed data stores While PACELC explains the main tradeoffs and distributed data stores, it doesn’t apply as-is to Kafka. Kafka is an event streaming platform used primarily for moving data from one place to another. A better way to look at a Kafka-based system is as a collection of producers, consumers, and topics forming distinct data flows. The consequence of that is, the client application configuration can influence every goal significantly and there are first class optimization goals in streaming applications such as end-to-end latency and throughput. Whether you are aware of it or not, every time you configure a Kafka cluster, create a topic, or configure a producer or a consumer, you are making a choice between latency vs throughput and availability vs durability. To explain why this is the case, we have to look into the main dimensions of Kafka topics and the role client applications play in an event flow.    Apache Kafka optimization goals PARTITIONS The topic partitions (on the X-axis in our diagram) represent the unit of parallelism in Kafka. On the broker, and the client-side, reads and writes to different partitions can be done fully in parallel. While there are many factors that can limit throughput, generally a higher number of partitions for a topic will enable higher throughput, and a smaller number of partitions will lead to lower throughput. At the same time, a very large number of partitions will lead to the creation of more metadata that needs to be passed and processed across all brokers and clients. This can impact end to end latency negatively unless more resources are added to the brokers. This is a purposely simplified view of partitions to demonstrate the main tradeoff that the number of partitions leads to in our context. REPLICAS The replication factor (on the Y-axis in our diagram) determines the number of copies (including the leader) each topic partition in a cluster must have. By ensuring all replicas of a partition exist on different brokers, replicas define the data durability. A higher number of replicas ensure the data is copied to more brokers and ensure better durability of data in the case of broker failure.  On the other hand, a lower number of replicas reduces the data durability but in certain circumstances it can increase the availability for the producers or consumers by tolerating more broker failures. That said, the availability for a consumer is determined by the availability of in-sync replicas, whereas for a producer it is determined by the minimum number of in-sync replicas. With a low number of replicas, the availability of overall data flow will be dependent on which brokers fail (the one with the partitions leader of interest or not) and if other brokers are in sync or not. For our purpose we assume less replicas would lead to higher application availability and lower data durability. Partitions and replicas are not the only primitives influencing the tradeoff between throughput vs latency and durability vs availability, but they represent the broker side of the picture. Other participants in shaping the main Kafka optimization tradeoffs are the client applications. PRODUCERS AND CONSUMERS Topics are used by the producers that send messages and consumers that read these messages. Producers and consumers also state their preference between throughput vs latency and durability vs availability through various configuration options. It is the combination of topic and client application configurations (and other cluster-level configurations such as leader election) that defines what your application is optimized for. We can look at a flow of events consisting of a producer, a consumer, and a topic. Optimizing such an event flow for average latency, on the client-side would require tuning the producer and consumer to exchange smaller batches of messages. The same flow can be tuned for average throughput by configuring the producer and consumer for larger batches of messages. In the same way, the number of topic partitions influences throughput vs latency, a producer and consumer message batch sizes influence the same. Producers and consumers involved in a message flow state preference for durability or availability too. A producer that favors durability over availability can demand a higher number of acknowledgements by specifying acks=all. A lower number of acknowledgments (lower than minISR which means 0 or 1) could lead to higher availability from the producer point of view by tolerating a higher number of broker failures, but reduces data durability in the case of catastrophic events with the brokers. The consumer configurations influencing this dimension (Y-axis) are not as straightforward as the producer configurations but dependent on the consumer application logic. A consumer can favor higher consistency by committing message consumption offsets more often or even individually. Or the consumer can favor availability by increasing various timeouts and tolerating broker failures for longer. KAFKA OPTIMIZATION THEOREM We defined the main actors involved in an event flow as a producer, a consumer, and a topic. We also defined the opposing goals we have to optimize for: throughput vs latency and durability vs availability. Given that, the Kafka Optimization Theorem states that any Kafka data flow makes tradeoffs between throughput vs latency and durability vs availability.   Kafka Optimization Theorem with primary configuration options For simplicity, we put producer and consumer preferences on the same axes as topic replicas and partitions. Optimizing for a particular goal would be easier to achieve when these primitives are aligned. For example, optimizing an end-to-end data flow for low latency would be best achieved with smaller producer and consumer message batches combined with a small number of partitions. A data flow optimized for higher throughput would have larger producer and consumer message batches and higher number of partitions for parallel processing. A data flow optimized for durability would have a higher number of replicas and require a higher number of producer acknowledgments and granular consumer commits. If the data flow is optimized for availability, a smaller number of replicas, and/or smaller number of producer acknowledgments, larger timeouts, would be preferred. In practice, there is no correlation between partitions, replicas, producers, and consumers configurations. It is possible to have a large number of replicas for a topic (minISR), but have a producer that requires a 0 or 1 of acknowledgment. Or have a higher number of partitions because your application logic requires so, and small producer and consumer message batches. These are valid scenarios to use Kafka and one of the strengths of Apache Kafka is being a highly configurable and flexible eventing system satisfying many use cases. The framework proposed here is to serve as a mental model for the main Kafka primitives and how they relate to the optimization dimensions. Knowing the foundational forces will enable you to do tuning specific for your application and understand the effects. THE GOLDEN RATIO  In the proposed Kafka Optimization Theorem, there are deliberately no numbers but only the relation between the main primitives and the direction of change. It is not intended to serve as a concrete optimization configuration but a guide that shows how reducing or increasing a primitive configuration influences the direction of the optimization. Yet sharing a few of the most common configuration options accepted as the industry best practices could be useful as a starting point and demonstration of the theorem in practice. Most Kafka clusters today, whether that is on-prem with something like , or as a fully managed service offering such as , are almost always deployed within a single region. A production-grade Kafka cluster is typically spread into 3 availability zones (AZs) with a replication factor of 3 (RF=3) and minimum in-sync replicas of 2 (minISR=2). This ensures a good level of data durability during happy times, and good availability for client applications during temporary disruptions. This represents the good middle ground as having minISR=3 would prevent producers from producing even when a single broker is affected, and having minISR=1 would affect both producer and consumer when a leader is down. Typically this replication configuration is accompanied by acks=all on the producer side and default offset commit configurations for the consumers. While there are commonalities in consistency and availability tradeoffs among different applications, throughput and latency requirements vary. The number of partitions for a topic is influenced by the shape of the data, the data processing logic, and its ordering requirements. At the same time, the number of partitions dictates what is the level of maximum parallelism and message throughput you can achieve. As a consequence, there is no good default number or a range for the partition count. By default, Kafka clients are optimized for low latency. We can observe that from the default producer values (batch.size=16384, linger.ms=0, compression.type=none) and the default consumer values (fetch.min.bytes=1, fetch.max.wait.ms=500). The producer prior Kafka 3 had acks=1 which recently changed to acks=all with a preference on durability rather than availability or low latency. Optimizing the client applications for throughput would require increasing wait times and batch sizes 5-10 fold and examining the results. Knowing the default values and what they are optimized for is a good starting point for your service optimization goal. SUMMARY CAP is a great theorem for failure scenarios. While failures are a given and partitioning will always happen, we have to optimize applications for happy paths too. This post introduces a simplified model explaining how Kafka primitives can be used for performance optimization. It deliberately focuses on the main primitives and selective configurations options to demonstrate the principles. In real life, more factors influence your application performance metrics. Encryption, compression, and memory allocation will impact latency and throughput. Transactions, exactly-one semantics, retries, message flushes, and leader election will impact consistency and availability tradeoffs. You will have broker primitives (replicas and partitions) and client primitives (batch sizes and acknowledgments) optimized for competing tradeoffs. Finding the ideal Kafka configuration for your application will require experimentation, and the Kafka Optimization Theorem will guide you in the journey. Follow me at to join my journey of learning Apache Kafka. This post was originally published on Red Hat Developers. To read the original post, check .</content><dc:creator>Unknown</dc:creator></entry><entry><title>Is your Go application FIPS compliant?</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/31/your-go-application-fips-compliant" /><author><name>Antonio Cardace</name></author><id>34f13f47-1141-4739-b60e-a6c4321d8297</id><updated>2022-05-31T07:40:00Z</updated><published>2022-05-31T07:40:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) ships with several Federal Information Processing Standards (FIPS)-validated cryptography libraries, including OpenSSL. This allows applications that use these libraries to operate in &lt;em&gt;FIPS mode,&lt;/em&gt; which means that the cryptographic techniques they use can are in compliance with the &lt;a href="https://www.encryptionconsulting.com/education-center/what-is-fips/"&gt;FIPS-140-2 standard&lt;/a&gt;. Any organization that works with the U.S. Federal government must comply with this standard.&lt;/p&gt; &lt;p&gt;By default, applications written in &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt; use cryptographic functions from the Go standard library, which is not FIPS-validated. However, the version of Go shipped in RHEL is based on upstream Go's dev.boringcrypto branch, which is modified to use BoringSSL for crypto primitives. Modifications made in the RHEL version replace BoringSSL with OpenSSL. These modifications allow applications written with RHEL's Go to use crypto functions from a FIPS-validated version of OpenSSL. This article will show you how to verify that your system, including your installation of the Go language, is capable of operating in FIPS mode.&lt;/p&gt; &lt;h2&gt;How to get started&lt;/h2&gt; &lt;p&gt;Begin by building your Go binary with the Go compiler shipped in RHEL. To do this quickly use the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/ubi-minimal/5c359a62bed8bd75a2c3fba8?gti-tabs=unauthenticated&amp;container-tabs=gtiimage"&gt;ubi8-minimal&lt;/a&gt; &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Universal Base Image&lt;/a&gt; (UBI) as your build environment and install the &lt;code&gt;go-toolset&lt;/code&gt; package.&lt;/p&gt; &lt;p&gt;To confirm that the Go compiler and built binary are FIPS-capable, run the command below, modified as appropriate for your environment (e.g, &lt;code&gt;$BINARY&lt;/code&gt; could be &lt;code&gt;/usr/bin/go&lt;/code&gt;):&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ go tool nm $BINARY | grep FIPS&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If the output looks like the listing below, with references to named &lt;code&gt;FIPS&lt;/code&gt; functions, then the binary is FIPS-capable. (If it weren't FIPS-capable, the output would be empty.)&lt;/p&gt; &lt;pre&gt; &lt;code&gt; [root@rhel-8-4 ~]# go tool nm ./main | grep FIPS 401210 T _cgo_23e85cd750d7_Cfunc__goboringcrypto_FIPS_mode 5d8d80 d _g_FIPS_mode 4c2680 T crypto/internal/boring._Cfunc__goboringcrypto_FIPS_mode 5a27c0 D crypto/internal/boring._cgo_23e85cd750d7_Cfunc__goboringcrypto_FIPS_mode &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Next, run the application in a container that includes a FIPS-compliant OpenSSL library. Again, you can use the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/ubi-minimal/5c359a62bed8bd75a2c3fba8?gti-tabs=unauthenticated&amp;container-tabs=gtiimage"&gt;ubi8-minimal image&lt;/a&gt;, which fulfills the requirement. If you're using another image, you can check to see if the OpenSSL installation is FIPS-capable with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ openssl version OpenSSL 1.1.1k FIPS 25 Mar 2021 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Note: This can be disabled by enforcing pure Go with a build time flag.&lt;/p&gt; &lt;p&gt;As we noted above, the version of Go that ships with RHEL is based on the upstream Go's dev.boringcrypto branch, which is modified to use BoringSSL. You can verify this by looking at the source code either in the Go source RPM file or in Fedora's source code manager.&lt;/p&gt; &lt;p&gt;Grepping over the Go source code shows the first signs of the patched-in BoringSSL support:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ grep -r boring /usr/lib/golang/src/crypto/ | head /usr/lib/golang/src/crypto/aes/cipher.go:import "crypto/internal/boring" /usr/lib/golang/src/crypto/aes/cipher.go: if boring.Enabled() { &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The &lt;a href="https://pagure.io/go/blob/go1.15-openssl-fips/f/src/crypto/internal/boring/boring.go#_14"&gt;crypto/internal/boring package&lt;/a&gt; includes directives that use CGO to dynamically link against &lt;code&gt;libdl&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; package boring // #include "goboringcrypto.h" // #cgo LDFLAGS: -ldl import "C" &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Linking against &lt;code&gt;libdl&lt;/code&gt; allows the use of the &lt;code&gt;dl_open()&lt;/code&gt; function, which allows for further loading of shared libraries. The &lt;code&gt;dl_open()&lt;/code&gt; function is called to &lt;a href="https://pagure.io/go/blob/go1.15-openssl-fips/f/src/crypto/internal/boring/goopenssl.h#_52"&gt;load libcrypto&lt;/a&gt;, one of the shared libraries in OpenSSL:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; static void* _goboringcrypto_DLOPEN_OPENSSL(void) { if (handle) { return handle; } #if OPENSSL_VERSION_NUMBER &lt; 0x10100000L handle = dlopen("libcrypto.so.10", RTLD_NOW | RTLD_GLOBAL); #else handle = dlopen("libcrypto.so.1.1", RTLD_NOW | RTLD_GLOBAL); #endif return handle; } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Because &lt;code&gt;dl_open()&lt;/code&gt; is used to load the OpenSSL libraries, rather than linking the actual Go binaries, &lt;code&gt;libcrypto&lt;/code&gt; will not appear in the output of &lt;code&gt;ldd&lt;/code&gt; when run on binaries built with RHEL Go (though &lt;code&gt;libdl&lt;/code&gt; will).&lt;/p&gt; &lt;p&gt;Although &lt;code&gt;libdl&lt;/code&gt; is linked, OpenSSL is not used by default. The &lt;a href="https://pagure.io/go/blob/go1.15-openssl-fips/f/src/crypto/internal/boring/boring.go#_55"&gt;crypto/internal/boring package&lt;/a&gt; will always load OpenSSL, but it will only use it if FIPS mode is enabled:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; func init() { runtime.LockOSThread() defer runtime.UnlockOSThread() // Check if we can `dlopen` OpenSSL if C._goboringcrypto_DLOPEN_OPENSSL() == C.NULL { return } // Initialize the OpenSSL library. C._goboringcrypto_OPENSSL_setup() // Check to see if the system is running in FIPS mode, if so // enable "boring" mode to call into OpenSSL for FIPS compliance. if fipsModeEnabled() { enableBoringFIPSMode() } sig.BoringCrypto() } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The &lt;code&gt;FIPS_mode_set()&lt;/code&gt; and &lt;code&gt;FIPS_mode()&lt;/code&gt; functions are defined in OpenSSL, which is why &lt;code&gt;libcrypto&lt;/code&gt; is loaded even if it won't be used for crypto functions. OpenSSL is used to check if the system is in FIPS mode.&lt;/p&gt; &lt;h2&gt;How to verify FIPS mode&lt;/h2&gt; &lt;p&gt;Depending on how deep you want to go, there are a couple of different ways in which you can check for FIPS compliance.&lt;/p&gt; &lt;h3&gt;Custom fips-detect tool&lt;/h3&gt; &lt;p&gt;A tool called &lt;a href="https://github.com/acardace/fips-detect"&gt;fips-detect&lt;/a&gt; is available to determine whether your system or container and your Golang binary are ready to run in FIPS mode. It accomplishes this by performing checks on the running system and the supplied binary to see if everything is in place to correctly run in FIPS mode.&lt;/p&gt; &lt;h3&gt;Common tools&lt;/h3&gt; &lt;p&gt;If you don't want to install a custom tool, you can use &lt;code&gt;ldd&lt;/code&gt; to show that Go binaries are linked against &lt;code&gt;libdl&lt;/code&gt;, and inspect the source code to determine if &lt;code&gt;dl_open()&lt;/code&gt; is used to load OpenSSL. There are a couple of options available, including &lt;code&gt;go tool nm&lt;/code&gt; or &lt;code&gt;readelf -s&lt;/code&gt;. However, keep in mind that no checks on the underlying system are performed with these common tools, and it may not be convincing enough to inspect the Go binaries alone.&lt;/p&gt; &lt;p&gt;If the binary is compiled on a standard Fedora 34 system, when you enter this &lt;code&gt;go tool nm&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ go tool nm ./main | grep -i dlopen_openssl &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The output you'll get will be blank. If the binary is compiled on RHEL 8, however, this is the result:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ go tool nm ./main | grep -i dlopen_openssl [root@rhel-8-4 ~]# go tool nm ./main | grep -i dlopen_openssl 4018d0 T _cgo_fb383f177a95_Cfunc__goboringcrypto_DLOPEN_OPENSSL &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;From this output, it is clear that, when compiling with the RHEL Golang compiler, the binary is at least able to call into OpenSSL and enable FIPS mode.&lt;/p&gt; &lt;p&gt;To verify that the program runs in FIPS mode, use the &lt;code&gt;LD_DEBUG=symbols&lt;/code&gt; environment variable. This shows the various symbols the binary binds from shared libraries to determine whether the application actually calls into OpenSSL.&lt;/p&gt; &lt;p&gt;The binary runs in FIPS mode when executed with the &lt;code&gt;OPENSSL_FORCE_FIPS_MODE=1&lt;/code&gt; variable. In the following example, we use a custom Go binary that computes a hash using the SHA1 algorithm:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; [root@rhel-8-4 ~]# env OPENSSL_FORCE_FIPS_MODE=1 ./main 5939: symbol=FIPS_mode; lookup in file=./main [0] 5939: symbol=FIPS_mode; lookup in file=/lib64/libssl.so.1.1 [0] 5939: symbol=FIPS_mode; lookup in file=/lib64/libcrypto.so.1.1 [0] 5939: symbol=SHA1_Init; lookup in file=./main [0] 5939: symbol=SHA1_Init; lookup in file=/lib64/libssl.so.1.1 [0] 5939: symbol=SHA1_Init; lookup in file=/lib64/libcrypto.so.1.1 [0] 5939: symbol=SHA1_Update; lookup in file=./main [0] 5939: symbol=SHA1_Update; lookup in file=/lib64/libssl.so.1.1 [0] 5939: symbol=SHA1_Update; lookup in file=/lib64/libcrypto.so.1.1 [0] 5939: symbol=SHA1_Final; lookup in file=./main [0] 5939: symbol=SHA1_Final; lookup in file=/lib64/libssl.so.1.1 [0] 5939: symbol=SHA1_Final; lookup in file=/lib64/libcrypto.so.1.1 [0] SHA1: C8282111D0FAD11680B3775A36E68DC41E36F911 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;For comparison, this is the result when it runs without this variable:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; [root@rhel-8-4 ~]# ./main 12957: symbol=FIPS_mode_set; lookup in file=/lib64/libcrypto.so.1.1 [0] 12957: symbol=dlsym; lookup in file=./main [0] 12957: symbol=dlsym; lookup in file=/lib64/libdl.so.2 [0] 12957: symbol=OPENSSL_init; lookup in file=/lib64/libcrypto.so.1.1 [0] 12957: symbol=FIPS_mode; lookup in file=/lib64/libcrypto.so.1.1 [0] SHA1: C8282111D0FAD11680B3775A36E68DC41E36F911 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The differences are:&lt;/p&gt; &lt;ul&gt; &lt;li aria-level="1"&gt;When the binary runs in non-FIPS mode, it uses the Golang standard crypto library, which uses its own routines (these are statically included in the binary) rather than calling into OpenSSL. This is why the dynamic linker doesn't bind OpenSSL symbols.&lt;/li&gt; &lt;li aria-level="1"&gt;OpenSSL is still loaded even when FIPS mode is disabled. It provides the functions used to check if FIPS mode is enabled or disabled. Only enabled FIPS mode uses OpenSSL for cryptographic functions.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;FIPS mode&lt;/h2&gt; &lt;p&gt;Earlier, we used the &lt;code&gt; OPENSSL_FORCE_FIPS_MODE&lt;/code&gt; environment variable to force the binary to behave as if FIPS mode were enabled.&lt;/p&gt; &lt;p&gt;The following is a snippet of the &lt;code&gt;libcrypto.so&lt;/code&gt; shared library constructor. It runs in the lib init phase before the dynamic linker transfers the flow control to &lt;code&gt;main()&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; # define FIPS_MODE_SWITCH_FILE "/proc/sys/crypto/fips_enabled" static void init_fips_mode(void) { char buf[2] = "0"; int fd; if (secure_getenv("OPENSSL_FORCE_FIPS_MODE") != NULL) { buf[0] = '1'; } else if ((fd = open(FIPS_MODE_SWITCH_FILE, O_RDONLY)) &gt;= 0) { while (read(fd, buf, sizeof(buf)) &lt; 0 &amp;&amp; errno == EINTR) ; close(fd); } if (buf[0] != '1' &amp;&amp; !FIPS_module_installed()) return; FIPS_mode_set(1); if (buf[0] != '1') { /* drop down to non-FIPS mode if it is not requested */ FIPS_mode_set(0); } else { /* abort if selftest failed */ FIPS_selftest_check(); } } void __attribute__ ((constructor)) OPENSSL_init_library(void) { static int done = 0; if (done) return; done = 1; init_fips_mode(); } &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This snippet comes from a patch included in the RHEL and Fedora versions of OpenSSL, so it only applies to RHEL and the UBI container image.&lt;/p&gt; &lt;p&gt;The shared library constructor shows that, to transparently enable FIPS mode, you must either define the &lt;code&gt;OPENSSL_FORCE_FIPS_MODE&lt;/code&gt; variable or ensure that the first byte of the &lt;code&gt;/proc/sys/crypto/fips_enabled&lt;/code&gt; file contains 1.&lt;/p&gt; &lt;p&gt;Containers will detect hosts that are in FIPS mode when &lt;code&gt;/proc/sys/crypto/fips_enabled&lt;/code&gt; appears with the host's value in all container PID namespaces.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Building Go applications on RHEL allows them to run in two different modes, default and FIPS. The default mode uses the Go standard library, and the FIPS mode uses a FIPS-validated version of OpenSSL. This provides developers an easy way to meet compliance requirements that mandate the use of FIPS-validated libraries, while also preserving consistency with applications built upstream.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/31/your-go-application-fips-compliant" title="Is your Go application FIPS compliant?"&gt;Is your Go application FIPS compliant?&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Antonio Cardace</dc:creator><dc:date>2022-05-31T07:40:00Z</dc:date></entry><entry><title>Integrate a Spring Boot application with Red Hat Data Grid</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/31/integrate-spring-boot-application-red-hat-data-grid" /><author><name>Alexander Barbosa Ayala</name></author><id>fd568e80-1fd7-4d83-be98-53ee6d73e237</id><updated>2022-05-31T07:00:00Z</updated><published>2022-05-31T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/data-grid" target="_blank"&gt;Red Hat Data Grid&lt;/a&gt; is a middleware solution that has been developed for application cache data storage. It makes it possible to access and process in-memory data, improving the end-user experience.&lt;/p&gt; &lt;p&gt;This article offers guidance for Spring Boot and Red Hat Data Grid integration on version 4.9 of the &lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift Container Platform&lt;/a&gt;. You will set up a Red Hat Data Grid 8.2 cluster and deploy a Spring Boot application with separate namespaces to use &lt;a href="https://infinispan.org/docs/stable/titles/hotrod_java/hotrod_java.html"&gt;Hot Rod&lt;/a&gt; communication between them.&lt;/p&gt; &lt;h2&gt;Setting up the environment&lt;/h2&gt; &lt;p&gt;The integration described in this article was made using the following technologies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Red Hat Data Grid 8.2&lt;/li&gt; &lt;li&gt;OpenShift 4.9&lt;/li&gt; &lt;li&gt;Spring Boot 2.5&lt;/li&gt; &lt;li&gt;Java 11&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Deploy the Data Grid cluster&lt;/h2&gt; &lt;p&gt;Refer to the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.2/html-single/data_grid_operator_guide/index#creating-minimal-clusters_infinispan-cr"&gt;Data Grid Operator Guide&lt;/a&gt; for step-by-step instructions on deploying Red Hat Data Grid. For this integration, the Data Grid project's name is &lt;code&gt;dgtest&lt;/code&gt;, and the Infinispan cluster's name is &lt;code&gt;infinispan-test&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Create the &lt;code&gt;dgtest&lt;/code&gt; project:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc new-project dgtest&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then create a new Data Grid cluster using the Operator. For this integration, we created a custom resource YAML file using the Operator option &lt;strong&gt;Create Infinispan.&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;apiVersion: infinispan.org/v1 kind: Infinispan metadata: name: infinispan-test namespace: dgtest spec: expose: type: Route service: type: DataGrid replicas: 2&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Gather relevant Data Grid cluster data&lt;/h2&gt; &lt;p&gt;To integrate the Data Grid cluster for external use, you'll need to save some cluster data, such as the developer user credentials, the cluster CRT certificate, and the local SVC DNS hostname.&lt;/p&gt; &lt;p&gt;Begin by getting the developer user credentials.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;oc get secret infinispan-test-generated-secret \ -o jsonpath="{.data.identities\.yaml}" | base64 --decode &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The credentials will look something like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;credentials: - username: developer password: SYtaUwZfPzNjHYeC roles: - admin &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now you need to get the data grid cluster certificate, &lt;code&gt;tls.crt&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;oc get secret infinispan-test-cert-secret \  -o jsonpath='{.data.tls\.crt}' | base64 --decode &gt; tls.crt &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Finally, get the service DNS hostname SVC for internal OpenShift routing.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;oc get service infinispan-test -o go-template --template='{{.metadata.name}}.{{.metadata.namespace}}.svc.cluster.local{{println}}' &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The result should be something like &lt;code&gt;infinispan-test.dgtest.svc.cluster.local&lt;/code&gt;, which is what we'll use as the hostname for the purposes of this article.&lt;/p&gt; &lt;h2&gt;Create a new cache&lt;/h2&gt; &lt;p&gt;For this integration, the application must be able to store and read cache data from a cache named &lt;code&gt;sessions&lt;/code&gt;. You can use &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.2/html/data_grid_server_guide/create_remote_cache" target="_blank"&gt;multiple methods&lt;/a&gt; to create the cache, including the command-line interface (CLI) and the Web Management console. This article will use the Data Grid CLI method, connecting through the exposed route and the &lt;code&gt;developer&lt;/code&gt; user credentials:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;./opt/infinispan/bin/cli.sh [disconnected]&gt; connect https://infinispan-test-dgtest.openshiftcluster.com/ --trustall Username: developer Password: **************** [infinispan-test-0-26062@infinispan-test//containers/default]&gt; create cache sessions --template=org.infinispan.DIST_SYNC [infinispan-test-0-26062@infinispan-test//containers/default]&gt; [infinispan-test-0-26062@infinispan-test//containers/default]&gt; quit &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The generated cache is as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat /opt/infinispan/server/data/caches.xml &lt;?xml version="1.0"?&gt; &lt;infinispan xmlns="urn:infinispan:config:13.0"&gt; &lt;cache-container&gt; &lt;caches&gt; &lt;distributed-cache name="sessions" mode="SYNC" remote-timeout="17500" statistics="true"&gt; &lt;locking concurrency-level="1000" acquire-timeout="15000" striping="false"/&gt; &lt;state-transfer timeout="60000"/&gt; &lt;/distributed-cache&gt; &lt;/caches&gt; &lt;/cache-container&gt; &lt;/infinispan&gt; &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;How to deploy the Spring Boot project&lt;/h2&gt; &lt;p&gt;The next step is to clone &lt;a href="https://github.com/alexbarbosa1989/hotrodspringboot"&gt;hotrodspringboot&lt;/a&gt;, a project that I have made available on my GitHub repository. This is a basic project that stores and retrieves cache data from the Data Grid cluster. It is deployed in a separate OpenShift namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone -b openshift https://github.com/alexbarbosa1989/hotrodspringboot &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You then need to create a truststore from the &lt;code&gt;tls.crt&lt;/code&gt; cluster certificate.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;keytool -importcert -keystore truststore.jks -alias server -file tls.crt &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The data from &lt;code&gt;truststore.jks&lt;/code&gt; allows the Spring Boot application to create the secure connection to the Data Grid cluster.&lt;/p&gt; &lt;p&gt;The sample Spring Boot application comes with a set of variables in the &lt;code&gt;application.properties&lt;/code&gt; file that must be replaced by the data previously gathered to make a correct connection with the Data Grid cluster. That &lt;code&gt;application.properties&lt;/code&gt; file is located in the &lt;code&gt;${project-home}/src/main/resources/&lt;/code&gt; directory.&lt;/p&gt; &lt;p&gt;So the next step is to update the variables in &lt;code&gt;application.properties&lt;/code&gt; file according to the Data Grid cluster data:&lt;/p&gt; &lt;table border="1" cellpadding="1" cellspacing="1" width="706"&gt; &lt;thead&gt; &lt;tr&gt; &lt;th scope="col"&gt;Variable&lt;/th&gt; &lt;th scope="col"&gt;Value&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$SERVICE_HOSTNAME&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;infinispan-test.dgtest.svc.cluster.local&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$CLUSTER_NAME&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;infinispan-test&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$USER_NAME&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;developer&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$USER_PASSWORD&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;SYtaUwZfPzNjHYeC&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$TRUST_STORE_FILE_PATH&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;/mnt/secrets/truststore.jks&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;$TRUST_STORE_PASSWORD&lt;/code&gt;&lt;/td&gt; &lt;td&gt;&lt;code&gt;password&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Once you've customized the &lt;code&gt;application.properties&lt;/code&gt; file with those mapped values, it should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt; # context-path server.servlet.context-path=/redhat # Connection infinispan.remote.server-list=infinispan-test.dgtest.svc.cluster.local infinispan.remote.client-intelligence=BASIC management.endpoints.web.exposure.include=* # Authentication infinispan.remote.use-auth=true infinispan.remote.sasl-mechanism=SCRAM-SHA-512 infinispan.remote.auth-realm=default infinispan.remote.auth-server-name=infinispan-test infinispan.remote.auth-username=developer infinispan.remote.auth-password=SYtaUwZfPzNjHYeC infinispan.remote.sasl_properties.javax.security.sasl.qop=auth # Encryption infinispan.remote.sni_host_name=infinispan-test.dgtest.svc.cluster.local infinispan.remote.trust_store_file_name=/mnt/secrets/truststore.jks infinispan.remote.trust_store_password=password infinispan.remote.trust_store_type=jks # Marshalling infinispan.remote.marshaller=org.infinispan.commons.marshall.JavaSerializationMarshaller infinispan.remote.java-serial-whitelist=com.redhat.hotrod.* &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Create a new project in OpenShift for Spring Boot application deployment&lt;/h2&gt; &lt;p&gt;Now it's time to create an OpenShift project for your Spring Boot application. Create a new project on your OpenShift cluster named &lt;code&gt;springboot-test&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc new-project springboot-test&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, create a secret with the previously created &lt;code&gt;truststore.jks&lt;/code&gt; file in the &lt;code&gt;springboot-test&lt;/code&gt; project:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc create secret generic truststore-secret --from-file=truststore.jks &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deploy the Spring Boot application&lt;/h2&gt; &lt;p&gt;Now you're ready to deploy the application on the &lt;code&gt;springboot-test&lt;/code&gt; OpenShift project.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc project springboot-test mvn clean fabric8:deploy -Popenshift &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Wait until the pod is up and running.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pods class="language-bash" NAME READY STATUS RESTARTS AGE hotrodspringboot-1-plkf7 1/1 Running 0 39s hotrodspringboot-s2i-1-build 0/1 Completed 0 92s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you need to add the secret to the deployment configuration. In this case, the mount path, &lt;code&gt;/mnt/secrets&lt;/code&gt;, must be the same as the one defined in the &lt;code&gt;application.properties&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc set volume dc/hotrodspringboot --add --name=truststore-secret -m /mnt/secrets/ -t secret --secret-name=truststore-secret --default-mode='0755'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Alternatively, you can load the secret using the Openshift web console by navigating to &lt;strong&gt;Projects -&gt; springboot-test -&gt; secrets -&gt; ap-secret -&gt; Add Secret to workload&lt;/strong&gt; (Figure 1).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_8.png?itok=NONq2tEL" width="600" height="304" alt="Screenshoot showing how to add a secret to the deployment configuration." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Add the secret to the deployment configuration. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;This process generates a new pod deployment after the new pod rollout is finished.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pods NAME READY STATUS RESTARTS AGE hotrodspringboot-2-deploy 0/1 Completed 0 100m hotrodspringboot-2-x7lgx 1/1 Running 0 100m hotrodspringboot-s2i-1-build 0/1 Completed 0 154m hotrodspringboot-s2i-2-build 0/1 Completed 0 102m &lt;/code&gt; &lt;/pre&gt; &lt;h2&gt;Test the deployed integration&lt;/h2&gt; &lt;p&gt;In order to test the service, you need to get the Spring Boot exposed route.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get routes NAME HOST/PORT hotrodspringboot hotrodspringboot-springboot-test.openshiftcluster.com PATH SERVICES PORT TERMINATION WILDCARD hotrodspringboot 8080 None &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Next, you need to create a new cache entry, which takes the form of a key/value pair. This application uses a REST endpoint to get the data to store in the cache. The context path used for the update service is &lt;code&gt; /update-cache/{cacheName}/{cacheKey}/{cacheValue}&lt;/code&gt;. Here is an example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X GET http://hotrodspringboot-springboot-test.openshiftcluster.com/redhat/update-cache/sessions/cacheKey1/cacheValue1 SUCCESS cacheValue1&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;You can repeat this process to create a second cache entry or however many key/value pairs that you want for your test:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X GET http://hotrodspringboot-springboot-test.openshiftcluster.com/redhat/update-cache/sessions/cacheKey2/cacheValue2 SUCCESS cacheValue2&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now you can get the values from any of the previously stored cache data. As was the case with the update service, the application contains a REST endpoint for querying data from the cache. You can make a query using the key name from any of the previously stored key/value pairs using the context path &lt;code&gt; /get-cache-value/{cacheName}/{cacheKey}&lt;/code&gt;, as in this example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X GET http://hotrodspringboot-springboot-test.openshiftcluster.com/redhat/get-cache-value/sessions/cacheKey1 The value for key: cacheKey1 is: cacheValue1&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;At this point, the Data Grid session cache stores both cache entries. You can also verify the entries on the Data Grid side.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[infinispan-test-0-26062@infinispan-test//containers/default]&gt; cache sessions [infinispan-test-0-26062@infinispan-test//containers/default/caches/sessions]&gt; stats { "time_since_start" : 5364, "time_since_reset" : 5364, "current_number_of_entries" : -1, "current_number_of_entries_in_memory" : -1, "total_number_of_entries" : 2, "off_heap_memory_used" : 0, "data_memory_used" : 0, "stores" : 2, &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can continue storing and querying data using these methods.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Data Grid integration allows applications to store and quickly use data externally. This example is a simple use case that can also apply to larger data sets across cloud and hybrid platforms. The official &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.3/html/data_grid_server_guide/index" target="_blank"&gt;product&lt;/a&gt; and &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.3/html/data_grid_operator_guide/index" target="_blank"&gt;Operator&lt;/a&gt; documentation provide more Data Grid features to fit your use case.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/31/integrate-spring-boot-application-red-hat-data-grid" title="Integrate a Spring Boot application with Red Hat Data Grid"&gt;Integrate a Spring Boot application with Red Hat Data Grid&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alexander Barbosa Ayala</dc:creator><dc:date>2022-05-31T07:00:00Z</dc:date></entry><entry><title>Banco do Brasil extracts Open Banking investment data with Quarkus and Kafka</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/banco-do-brasil-open-banking-user-story/&#xA;            " /><author><name>Felipe Henrique Gross Windmoller (https://twitter.com/felipewind83)</name></author><id>https://quarkus.io/blog/banco-do-brasil-open-banking-user-story/</id><updated>2022-05-31T00:00:00Z</updated><published>2022-05-31T00:00:00Z</published><summary type="html">Banco do Brasil S.A. is a Brazilian financial services company headquartered in Brasília, Brazil. The oldest bank in Brazil, and among the oldest banks in continuous operation in the world, it was founded by John VI, King of Portugal, in 1808. It is the second largest banking institution in Brazil,...</summary><dc:creator>Felipe Henrique Gross Windmoller (https://twitter.com/felipewind83)</dc:creator><dc:date>2022-05-31T00:00:00Z</dc:date></entry><entry><title type="html">Infinispan 14.0.0.Dev03</title><link rel="alternate" href="https://infinispan.org/blog/2022/05/30/infinispan-14" /><author><name>Tristan Tarrant</name></author><id>https://infinispan.org/blog/2022/05/30/infinispan-14</id><updated>2022-05-30T12:00:00Z</updated><content type="html">Dear Infinispan community, Infinispan 14 development release 03 is here! We plan to release Infinispan 14 Final this summer, so we want to share a preview of what’s coming. JAVA 11 Infinispan now requires Java 11 to run (it was only needed to build it before). This means we can use and expose all of the great new APIs that were added, such as java.util.concurrent.Flow which provides a standard interface for all things reactive. Which brings us to the… NEW API We have finalized the design of our new user-facing API, which brings the following, much-needed, features: * a common API for both embedded and remote * clean separation between sync and async APIs, as well as a variant which blends beautifully with all the great things happening over in . * a single entry-point to access all of the data-structures that we support (caches, counters, locks, multimaps, etc) * our own annotations for indexing entity fields (see below for details) We are now working on implementing this API for the remote Hot Rod client, while the implementation for embedded will be available in Infinispan 15. JGROUPS 5 Upgrading to Java 11 also allows us to upgrade to JGroups 5.x, which brings a bunch of improvements: * Improved failure-detection protocols (FD_ALL3, FD_SOCK2) * The Random Early Drop protocol (RED), which starts dropping messages on the send side when the queue becomes full to prevent message storms caused by unneeded retransmissions. * Lots more. GROUPING The grouping API has a small improvement when searching for keys belonging to a group. The old code was inefficient because it iterates over all keys in the local nodes but that was changed in this release by iterating over a single segment. TRANSACTIONAL CACHES The internal codeis now non-blocking, reducing the overall threads spawning and making better use of resources when transactions are committed. CROSS-SITE REPLICATION The asynchronous cross-site replication updates are batched in the sender improving the overall resources utilization. JAKARTA EE JavaEE is dead. Long-live . Wherever we used to depend on javax APIs, we now depend on their jakarta equivalent. We still provide compatibility artifacts for legacy deployments. HIBERNATE ORM 6.0 COMPATIBILITY Infinispan’s Hibernate ORM second-level cache (2LC) implementation has been upgraded to work with Hibernate 6.0. INDEXING AND QUERY Lots has been happening in the land of indexing and querying. * Upgraded Hibernate Search to 6.1 and Lucene 8.11. * Brand-new annotations for indexing annotations in place of the old Hibernate annotations. * Schema index update to acquire ProtoBuf schema backward-compatible changes without touching the pre-existing index data. * The removal of the @ProtoDoc annotation to wrap indexing annotations for ProtoBuf generation. * New indexing startup mode configuration, to trigger purge or reindex automatically when the cache starts. * Support pagination for unbounded result size queries with the HotRod client. * Support query parameters for full-text analyzed fields. * Support normalizers with the HotRod client. * Improve the Hybrid query system. MICROMETER We’ve replaced our use of SmallRye Metrics (an implementation of Microprofile Metrics), with the much better Micrometer. SERVER * RESP endpoint: a Redis-compatible endpoint connector (implementing the RESP 3 protocol) with support for a subset of commands: set, get, del, mget, mset, incr, decr, publish, subscribe, auth, ping. The connector integrates with our security and protocol auto-detections, so that it is easily usable from our single-port endpoint. The implemented commands should be enough for basic usage. If you would like to see more, reach out via our community. * Support for FIPS environments (PKCS#11) * Support for masked and external credentials CONSOLE The console now sports a cache-creation wizard: a feature-driven approach to configuring caches just the way you need them. DOCUMENTATION As always, the Infinispan team hope you find the documentation useful and complete. We’d love to hear from you and really value feedback from our community. If you think something is missing from the docs or spot a correction, please get in touch and we’ll get on it straight away. RELEASE NOTES You can look at the to see what has changed. OTHER RELEASES We’ve also just recently updated our stable releases with important fixes: * * Get them from our .</content><dc:creator>Tristan Tarrant</dc:creator></entry><entry><title>Red Hat Developer roundup: Best of May 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/30/red-hat-developer-roundup-best-may-2022" /><author><name>Red Hat Developer Editorial Team</name></author><id>5c132813-fe79-494b-8008-ce380725f246</id><updated>2022-05-30T07:00:00Z</updated><published>2022-05-30T07:00:00Z</published><summary type="html">&lt;p&gt;Welcome to our monthly recap of the articles we published in May! In case you missed it, we had some important product announcements this month that deserve your attention:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/26/orchestrate-offloaded-network-functions-dpus-red-hat-openshift"&gt;Orchestrate offloaded network functions on DPUs with Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/whats-new-openshift-local-20"&gt;What’s new in OpenShift Local 2.0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/access-rhel-developer-teams-subscription"&gt;Access RHEL with a Developer for Teams subscription&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/introducing-red-hat-openshift-extension-docker-desktop"&gt;Introducing Red Hat OpenShift extension for Docker Desktop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/11/rhel-86-whats-new-and-how-upgrade"&gt;RHEL 8.6: What's new and how to upgrade&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/12/developer-tools-rebrand-say-farewell-codeready-name"&gt;Developer tools rebrand, say farewell to CodeReady name&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/whats-new-red-hat-enterprise-linux-9"&gt;What's new in Red Hat Enterprise Linux 9&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/26/whats-new-ansible-automation-platform-22"&gt;What's new in Ansible Automation Platform 2.2&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And of course, we rolled out a plethora of articles to help you write code on the platforms you trust. Here are the May highlights.&lt;/p&gt; &lt;ul&gt; &lt;/ul&gt; &lt;h2&gt;Understanding the Kafka landscape&lt;/h2&gt; &lt;p&gt;Bilgim Ibryam unleashed a series of popular Kafka articles this month, starting with &lt;a href="https://developers.redhat.com/articles/2022/05/03/fine-tune-kafka-performance-kafka-optimization-theorem"&gt;Fine-tune Kafka performance with the Kafka optimization theorem&lt;/a&gt;, which analyzes the options you need to consider with any Kafka rollout, balancing latency against throughput and durability against availability.&lt;/p&gt; &lt;p&gt;He also delivered a two-part series analyzing the different types of Kafka distribution on the landscape, looking at both &lt;a href="https://developers.redhat.com/articles/2022/05/16/all-about-local-and-self-managed-kafka-distributions"&gt;local and self-managed Kafka distributions&lt;/a&gt; as well as &lt;a href="https://developers.redhat.com/articles/2022/05/24/managed-kafka-services-which-right-you"&gt;managed services&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To get the latest on what's new in the Kafka world, be sure to check our &lt;a href="https://developers.redhat.com/articles/2022/05/09/kafka-monthly-digest-april-2022"&gt;Kafka monthly digest&lt;/a&gt; series.&lt;/p&gt; &lt;h2&gt;OpenShift and AWS&lt;/h2&gt; &lt;p&gt;Kubernetes and &lt;a href="https://developers.redhat.com/products/openshift/getting-started"&gt;Red Hat OpenShift&lt;/a&gt; can play well with Amazon Web Services cloud environments. August Simonelli brought you a two-part series showing how to simplify the management of services offered for Kubernetes by AWS:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/16/how-use-operators-aws-controllers-kubernetes"&gt;How to use Operators with AWS Controllers for Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/24/create-aws-resources-kubernetes-and-operators"&gt;Create AWS resources with Kubernetes and Operators&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And for a deep dive on managing secure connections with OpenShift and AWS, read &lt;a href="https://developers.redhat.com/articles/2022/04/27/create-privatelink-red-hat-openshift-cluster-aws-sts"&gt;Create a PrivateLink Red Hat OpenShift cluster on AWS with STS&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;SaaS architectures&lt;/h2&gt; &lt;p&gt;This month we published the first two articles in a new series about building and deploying SaaS applications, with a focus on software and deployment architectures:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/09/approaches-implementing-multi-tenancy-saas-applications"&gt;A SaaS architecture checklist for Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes"&gt;Approaches to implementing multi-tenancy in SaaS applications&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check these articles out and be on the lookout for future installments.&lt;/p&gt; &lt;h2&gt;Instrument containerized Java applications with Cryostat&lt;/h2&gt; &lt;p&gt;&lt;a href="https://cryostat.io"&gt;Cryostat&lt;/a&gt; is a tool for managing &lt;a href="https://docs.oracle.com/javacomponents/jmc-5-4/jfr-runtime-guide/about.htm#JFRUH170"&gt;JDK Flight Recorder&lt;/a&gt; data on &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, and Cryostat 2.1 delivers a slew of improvements and new features. Check out this series on the topic to learn more:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/sso-all-cryostats-new-openshift-login-flow"&gt;How to log into Cryostat 2.1 on OpenShift: SSO for all&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/11/how-build-automated-jfr-rules-cryostat-21s-new-ui"&gt;How to build automated JFR rules with Cryostat 2.1's new UI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/12/how-organize-jfr-data-recording-labels-cryostat-21"&gt;How to organize JFR data with recording labels in Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/17/manage-jfr-across-instances-cryostat-and-graphql"&gt;Manage JFR across instances with Cryostat and GraphQL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/19/manage-jmx-credentials-kubernetes-cryostat-21"&gt;Manage JMX credentials on Kubernetes with Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/25/eat-fewer-resources-cryostat-21-sidecar-reports"&gt;Eat up fewer resources in Cryostat 2.1 with sidecar reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/25/access-jfr-data-faster-cryostat-21s-new-download-apis"&gt;Access JFR data faster with Cryostat 2.1's new download APIs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/26/filter-unwanted-notifications-cryostat-21"&gt;Filter unwanted notifications in Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;May 2022 on Red Hat Developer&lt;/h2&gt; &lt;p&gt;Here's the full lineup of articles published on Red Hat Developer so far this month:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/26/orchestrate-offloaded-network-functions-dpus-red-hat-openshift"&gt;Orchestrate offloaded network functions on DPUs with Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/27/create-privatelink-red-hat-openshift-cluster-aws-sts"&gt;Create a PrivateLink Red Hat OpenShift cluster on AWS with STS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/28/use-red-hats-single-sign-technology-secure-services-through-kerberos"&gt;Use Red Hat's single sign-on technology to secure services through Kerberos&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/28/process-formula-1-telemetry-quarkus-and-openshift-streams-apache-kafka"&gt;Process Formula 1 telemetry with Quarkus and OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/02/no-code-and-low-code-integrations-camel-and-kaoto"&gt;No-code and low-code integrations with Camel and Kaoto&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/02/podman-basics-resources-beginners-and-experts"&gt;Podman basics: Resources for beginners and experts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/03/fine-tune-kafka-performance-kafka-optimization-theorem"&gt;Fine-tune Kafka performance with the Kafka optimization theorem&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/03/red-hat-cloud-way-event-driven-serverless-distributed-cloud-services-support"&gt;The Red Hat Cloud way: Event-driven, serverless, distributed cloud services to support modern apps&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/04/schedule-tests-gitops-way-testing-farm-github-action"&gt;Schedule tests the GitOps way with Testing Farm as GitHub Action&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/04/use-red-hats-sso-manage-kafka-broker-authorization"&gt;Use Red Hat's SSO to manage Kafka broker authorization&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/05/how-install-open-source-tool-creating-machine-learning-pipelines"&gt;How to install an open source tool for creating machine learning pipelines&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/05/build-customized-developer-portal-manage-apis"&gt;Build a customized developer portal to manage APIs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/09/using-unsafe-safely-graalvm-native-image"&gt;Using Unsafe safely in GraalVM Native Image&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/09/kafka-monthly-digest-april-2022"&gt;Kafka Monthly Digest: April 2022&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/sso-all-cryostats-new-openshift-login-flow"&gt;How to log into Cryostat 2.1 on OpenShift: SSO for all&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/introducing-red-hat-openshift-extension-docker-desktop"&gt;Introducing Red Hat OpenShift extension for Docker Desktop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/access-rhel-developer-teams-subscription"&gt;Access RHEL with a Developer for Teams subscription&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/sso-all-cryostats-new-openshift-login-flow"&gt;How to log into Cryostat 2.1 on OpenShift: SSO for all&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/6-design-tips-java-microservices-development"&gt;6 design tips for Java microservices development&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/improve-developer-experience-and-process-local-kubernetes"&gt;Improve the developer experience and process, from local to Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/manage-cluster-resources-efficiently-red-hat-openshift"&gt;Manage cluster resources efficiently with Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/access-rhel-developer-teams-subscription"&gt;Access RHEL with a Developer for Teams subscription&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/10/introducing-red-hat-openshift-extension-docker-desktop"&gt;Introducing Red Hat OpenShift extension for Docker Desktop&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/11/how-build-automated-jfr-rules-cryostat-21s-new-ui"&gt;How to build automated JFR rules with Cryostat 2.1's new UI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/11/rhel-86-whats-new-and-how-upgrade"&gt;RHEL 8.6: What's new and how to upgrade&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/12/how-organize-jfr-data-recording-labels-cryostat-21"&gt;How to organize JFR data with recording labels in Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/12/developer-tools-rebrand-say-farewell-codeready-name"&gt;Developer tools rebrand, say farewell to CodeReady name&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/16/how-use-operators-aws-controllers-kubernetes"&gt;How to use Operators with AWS Controllers for Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/16/all-about-local-and-self-managed-kafka-distributions"&gt;All about local and self-managed Kafka distributions&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/17/manage-jfr-across-instances-cryostat-and-graphql"&gt;Manage JFR across instances with Cryostat and GraphQL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes"&gt;A SaaS architecture checklist for Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/18/whats-new-red-hat-enterprise-linux-9"&gt;What's new in Red Hat Enterprise Linux 9&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/19/manage-jmx-credentials-kubernetes-cryostat-21"&gt;Manage JMX credentials on Kubernetes with Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/09/approaches-implementing-multi-tenancy-saas-applications"&gt;Approaches to implementing multi-tenancy in SaaS applications&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/23/how-install-command-line-tools-mac"&gt;How to install command-line tools on a Mac&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/23/plpmtud-delivers-better-path-mtu-discovery-sctp-linux"&gt;PLPMTUD delivers better path MTU discovery for SCTP in Linux&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/24/create-aws-resources-kubernetes-and-operators"&gt;Create AWS resources with Kubernetes and Operators&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/24/managed-kafka-services-which-right-you"&gt;Managed Kafka services: Which is right for you?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/25/eat-fewer-resources-cryostat-21-sidecar-reports"&gt;Eat up fewer resources in Cryostat 2.1 with sidecar reports&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/25/access-jfr-data-faster-cryostat-21s-new-download-apis"&gt;Access JFR data faster with Cryostat 2.1's new download APIs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/26/filter-unwanted-notifications-cryostat-21"&gt;Filter unwanted notifications in Cryostat 2.1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/26/whats-new-ansible-automation-platform-22"&gt;What’s new in Ansible Automation Platform 2.2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/05/26/experiment-openshift-api-management-developer-sandbox"&gt;Experiment with the OpenShift API Management Developer Sandbox&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/30/red-hat-developer-roundup-best-may-2022" title="Red Hat Developer roundup: Best of May 2022"&gt;Red Hat Developer roundup: Best of May 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Red Hat Developer Editorial Team</dc:creator><dc:date>2022-05-30T07:00:00Z</dc:date></entry><entry><title type="html">How to run WildFly on Openshift</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/openshift/using-wildfly-on-openshift/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/openshift/using-wildfly-on-openshift/</id><updated>2022-05-30T00:38:00Z</updated><content type="html">This tutorial will teach you how to run WildFly applications on Openshift using WildFly S2I images. At first, we will learn how to build and deploy applications using Helm Charts. Then, we will learn how to use the S2I legacy approach which relies on ImageStreams and Templates. WildFly Cloud deployments There are two main strategies ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">New Keycloak certifications</title><link rel="alternate" href="https://www.keycloak.org/2022/05/oidc-certifications" /><author><name>Marek Posolda</name></author><id>https://www.keycloak.org/2022/05/oidc-certifications</id><updated>2022-05-30T00:00:00Z</updated><content type="html">We are glad to announce new certifications for Keycloak related to the and ! In the , we announced certification of Keycloak 15.0.2 with the FAPI and Brazil Open Banking. This is a follow-up of this post with the announcement of the additional certifications. Here are the details: * Keycloak 18.0.0 is re-certified as OpenID Connect Provider. We already obtained certification for the OpenID Connect protocol a long time ago with the Keycloak 2.3.0. We now re-certified all the existing configurations (Basic, Implicit, Hybrid, Config, Dynamic) with latest Keycloak 18.0.0 and added certification as a Form Post OP. See the for the details. * Keycloak 18.0.0 is certified as OpenID Connect Logout Provider with all logout profiles (RP-Initiated OP, Session OP, Front-Channel OP, Backchannel OP). See the for the details. * Keycloak 15.0.2 is certified as , which is the extension based on existing FAPI 1 Advanced Final certification, which Keycloak already obtained before. See the for the details. This milestone was achieved due the hard work of the awesome Keycloak community, who contributed lots of features related to OpenID Connect Protocol, OpenID Connect Logout and FAPI. The special Thanks go to the , who helped a lot with the FAPI and OpenID Connect related features and especially to , who is doing an awesome job for the Keycloak project.</content><dc:creator>Marek Posolda</dc:creator></entry><entry><title>What’s new in Ansible Automation Platform 2.2</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/05/26/whats-new-ansible-automation-platform-22" /><author><name>Don Schenck</name></author><id>b36c1da4-5a9c-4beb-a73b-271f93dd4dcf</id><updated>2022-05-26T15:30:00Z</updated><published>2022-05-26T15:30:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; is a Red Hat offering based on &lt;a href="https://www.ansible.com/overview/it-automation"&gt;Ansible&lt;/a&gt; that allows you to configure your systems via code, and version 2.2 is now generally available. Whether you want to install a framework, deploy an application, or tweak some network settings, Ansible Automation Platform is the easiest way to get the job done.&lt;/p&gt; &lt;p&gt;Ansible Automation Platform works with &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; too, which means you can bring network administration experience from virtual machine (VM) environments to the brave new world of &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Here's a list of what's new for Ansible Automation Platform 2.2:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A topology viewer&lt;/li&gt; &lt;li&gt;More powerful developer tooling&lt;/li&gt; &lt;li&gt;Enhancements to network automation&lt;/li&gt; &lt;li&gt;Streamlined integration with &lt;a href="https://www.redhat.com/en/technologies/management/insights"&gt;Red Hat Insights&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Evolving support in &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Chain-of-custody tracking via digitally-signed components&lt;/li&gt; &lt;li&gt;An automation services catalog as a self-hosted, on-premises offering&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let's dive into the details for each of these.&lt;/p&gt; &lt;h2&gt;Topology viewer&lt;/h2&gt; &lt;p&gt;The Ansible Automation Platform topology viewer (Figure 1) allows you to see your overlay network and the results of Ansible automation. The visual layout makes support and troubleshooting easier while providing you with a big-picture view.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig1_7.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig1_7.png?itok=EtrLIb_5" width="367" height="388" alt="Screenshot of the topology viewer, which shows the relationships between your resources." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The topology viewer shows the relationships between your resources. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;More powerful developer tooling&lt;/h2&gt; &lt;p&gt;Ansible now has ansible-lint, which works like any good code linter to highlight errors, promote best practices, and reduce mistakes. ansible-lint also acts as an assistant if you're upgrading from an older (1.2) version of Ansible Automation Platform—which has an end-of-life date of September 2023.&lt;/p&gt; &lt;p&gt;There's also ansible-navigator 2.0, which brings other added capabilities to the table as well:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pass-through control of ansible-builder, along with experimental pass-through control of ansible-lint&lt;/li&gt; &lt;li&gt;Simple settings management of modified and active ansible-navigator configurations within Visual Studio Code&lt;/li&gt; &lt;li&gt;Native execution of ad hoc Ansible commands in an execution environment&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To make playbook development easier, a Visual Studio Code extension is &lt;a href="https://marketplace.visualstudio.com/items?itemName=redhat.ansible"&gt;available for download&lt;/a&gt; (Figure 2). As you would expect, this extension does syntax highlighting and real-time code validation as you type. Autocompletion is built in, and the extension has ansible-lint integrated as well.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/vx.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/vx.png?itok=g8IHqw-M" width="645" height="298" alt="The Visual Studio Marketplace offers a VS Code extension for Ansible." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The Visual Studio Marketplace offers a VS Code extension for Ansible. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Visual Studio Marketplace offers a Visual Studio Code extension for Ansible.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Enhancements to network automation&lt;/h2&gt; &lt;p&gt;Ansible Automation Platform 2.2 brings better performance and resilience to your automation tasks. &lt;a href="https://www.libssh.org"&gt;libssh,&lt;/a&gt; which uses the pylibSSH library, is now used by default for SSH connections.&lt;/p&gt; &lt;p&gt;This new release also uses direct execution by default to improve performance. This means that commands are carried out by the Ansible control node instead of being packaged and executed by the shell.&lt;/p&gt; &lt;p&gt;New resource modules have been released, including &lt;code&gt;snmp_server&lt;/code&gt; and &lt;code&gt;hostname&lt;/code&gt; modules for supported network operating systems: Arista, Cisco, Juniper, and VyOS.&lt;/p&gt; &lt;h2&gt;Streamlined integration with Red Hat Insights&lt;/h2&gt; &lt;p&gt;Ansible Automation Platform 2.2 includes a simpler, more intuitive way to connect your automation data with Red Hat Insights. The insights-client package is responsible for ensuring that connected data for your Ansible Automation Platform infrastructure has also been added to the bundled installer on the Red Hat Customer Portal.&lt;/p&gt; &lt;p&gt;Once you are connected with Red Hat Insights, you can get actionable metrics and dashboards to help identify, troubleshoot, and resolve operational, business, and security issues across your entire ecosystem.&lt;/p&gt; &lt;p&gt;You also gain full visibility into the performance and return on investment (ROI) of your efforts, helping you make more informed decisions to optimize and expand your automation.&lt;/p&gt; &lt;h2&gt;Evolving support in Red Hat Enterprise Linux&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux is keeping up with Ansible Automation Platform in a number of ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ansible Automation Platform 2.2 components are now available in the Red Hat Customer Portal as RPM packages, in both versions 8 and 9 of Red Hat Enterprise Linux.&lt;/li&gt; &lt;li&gt;Ansible Automation Platform 2.2 brings support for version 13 of the PostgreSQL database, which is in Red Hat Enterprise Linux 9. PostgreSQL 13 can be used by multiple Ansible Automation Platform components for improved compatibility and performance.&lt;/li&gt; &lt;li&gt;Ansible Automation Platform 2.2 includes an updated certified Red Hat Enterprise Linux System Roles Collection to automate Red Hat Enterprise Linux 9 instances.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;An eye toward the future…&lt;/h2&gt; &lt;p&gt;The following two features have been introduced in Ansible Automation Platform 2.2 as technology previews. These features provide early access to upcoming product innovations.&lt;/p&gt; &lt;h3&gt;Chain-of-custody tracking via digitally-signed components&lt;/h3&gt; &lt;p&gt;Digital signing of content is an important security feature that allows you to assure that the chain of custody for your assets stays within trusted providers.&lt;/p&gt; &lt;p&gt;Red Hat is also announcing Red Hat Ansible Certified Content: Digitally-signed content from Red Hat and partners to provide end-to-end security from download to deployment.&lt;/p&gt; &lt;p&gt;You can also sign your own content when publishing it to your private automation hub.&lt;/p&gt; &lt;h3&gt;Automation services catalog as a self-hosted, on-premises offering&lt;/h3&gt; &lt;p&gt;The automation services catalog is now a self-hosted, on-premises (private) offering. This gives automation creators and business users self-service access across physical, virtual, cloud, container, and edge environments.&lt;/p&gt; &lt;p&gt;This new iteration of the automation services catalog helps organizations extend the value of their automation to the business user by presenting access to Ansible Automation Platform in a catalog-style format.&lt;/p&gt; &lt;p&gt;With multilevel approval and role-based access control (RBAC), administrators can deploy projects more quickly, with the governance they need to meet compliance and procurement requirements.&lt;/p&gt; &lt;h2&gt;Make the move to 2.2&lt;/h2&gt; &lt;p&gt;You can download the new version of Ansible Automation Platform from &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;our Ansible website&lt;/a&gt;. Try out the newest version and automate everything you do.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/05/26/whats-new-ansible-automation-platform-22" title="What’s new in Ansible Automation Platform 2.2"&gt;What’s new in Ansible Automation Platform 2.2&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2022-05-26T15:30:00Z</dc:date></entry></feed>
